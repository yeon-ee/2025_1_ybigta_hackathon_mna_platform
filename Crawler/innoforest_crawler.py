import os
import json
import time
import random
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By

class InnoForestCrawler:
    def __init__(self):
        base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        self.dataset_dir = os.path.join(base_dir, 'Dataset')
        os.makedirs(self.dataset_dir, exist_ok=True)

        self.output_file = os.path.join(self.dataset_dir, 'inno_company.json')
        self.error_log_file = os.path.join(self.dataset_dir, 'error_log.json')
        self.result = []
        self.error_list = []
        if os.path.exists(self.output_file):
            with open(self.output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                self.existing_urls = set([comp["url"] for comp in json.loads(content)]) if content.strip() else set()
        else:
            self.existing_urls = set()

        chrome_driver_path = r"C:\Users\kimhy\chromedriver-win64\chromedriver.exe"
        service = Service(executable_path=chrome_driver_path)

        options = Options()
        options.add_argument(r"user-data-dir=C:\Users\kimhy\AppData\Local\Google\Chrome\User Data")
        options.add_argument("profile-directory=Profile 3")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)

        self.driver = webdriver.Chrome(service=service, options=options)
        print("ğŸš€ í¬ë¡¬ ì„¸ì…˜ ë¡œë”© ì™„ë£Œ (ë¡œê·¸ì¸ ìœ ì§€)")

    def random_sleep(self, a=0.5, b=1.5):
        time.sleep(random.uniform(a, b))

    def get_dd(self, label):
        self.random_sleep()
        try:
            dt_elem = self.driver.find_element(By.XPATH, f"//dt[contains(text(), '{label}')]")
            dd_elem = dt_elem.find_element(By.XPATH, "following-sibling::dd[1]")
            text = dd_elem.text.strip()
            return text if text != "-" and text else None
        except:
            return None

    def parse_major_info(self):
        self.random_sleep()
        info = {}
        try:
            keys_to_get = ["ìë³¸ê¸ˆ", "ê³ ìš©ì¸ì›", "ëˆ„ì íˆ¬ììœ ì¹˜ê¸ˆì•¡", "íˆ¬ììœ ì¹˜ê±´ìˆ˜", "ì—°ë§¤ì¶œ", "ê¸°ìˆ ë“±ê¸‰"]
            blocks = self.driver.find_elements(By.CSS_SELECTOR, "div.css-1s5aaxq > dl > div")
            for block in blocks:
                key = block.find_element(By.CSS_SELECTOR, "dt").text.strip()
                if key in keys_to_get:
                    val = block.find_element(By.CSS_SELECTOR, "dd").text.strip()
                    info[key] = val if val != "-" and val else None
        except:
            pass
        return info

    def parse_table(self, label):
        self.random_sleep()
        data = {}
        try:
            bars = self.driver.find_elements(By.CSS_SELECTOR, f"div#{label}Chart g[aria-label]")
            for bar in bars:
                aria = bar.get_attribute("aria-label")
                parts = aria.split()
                if len(parts) >= 3:
                    item, year, value = parts[0], parts[1], parts[2]
                    data.setdefault(item, {})[year] = value
        except:
            pass
        return data

    def parse_news(self):
        self.random_sleep()
        news = []
        try:
            news_list = self.driver.find_elements(By.CSS_SELECTOR, "ul.css-1nx5s0 li")
            for n in news_list:
                try:
                    link = n.find_element(By.TAG_NAME, "a").get_attribute("href")
                    title = n.find_element(By.TAG_NAME, "dt").text.strip()
                    date = n.find_element(By.TAG_NAME, "dd").text.strip()
                    news.append({"ë§í¬": link, "ì œëª©": title, "ë‚ ì§œ": date})
                except:
                    continue
        except:
            pass
        return news

    def parse_patents(self):
        self.random_sleep()
        patents = []
        try:
            items = self.driver.find_elements(By.CSS_SELECTOR, "ul.css-tgpw9p > li")
            for item in items:
                try:
                    title = item.find_element(By.TAG_NAME, "dt").text.split("{")[0].strip()
                    patents.append(title)
                except:
                    continue
        except:
            pass
        return patents
    
    def parse_investment(self):
        invest_info = {
        "ìµœì¢…íˆ¬ìë‹¨ê³„": None,
        "ëˆ„ì íˆ¬ììœ ì¹˜ê¸ˆì•¡": None,
        "íˆ¬ììœ ì¹˜ê±´ìˆ˜": None,
        "íˆ¬ììœ ì¹˜ì´ë ¥": []
    }
        try:
            invest_info["ìµœì¢…íˆ¬ìë‹¨ê³„"] = self.get_dd("ìµœì¢…íˆ¬ìë‹¨ê³„")
            invest_info["ëˆ„ì íˆ¬ììœ ì¹˜ê¸ˆì•¡"] = self.get_dd("ëˆ„ì íˆ¬ììœ ì¹˜ê¸ˆì•¡")
            invest_info["íˆ¬ììœ ì¹˜ê±´ìˆ˜"] = self.get_dd("íˆ¬ììœ ì¹˜ê±´ìˆ˜")
            rows = self.driver.find_elements(By.CSS_SELECTOR, "div.css-1ipakji table tr")[1:]
            for row in rows:
                cells = row.find_elements(By.TAG_NAME, "td")
                if len(cells) >= 3:
                    date = cells[0].text.strip()
                    stage = cells[1].text.strip()
                    amount = cells[2].text.strip()
                    invest_info["íˆ¬ììœ ì¹˜ì´ë ¥"].append({"ë‚ ì§œ": date, "ë‹¨ê³„": stage, "ê¸ˆì•¡": amount})
        except:
            pass
        return invest_info


    def parse_categories(self):
        self.random_sleep()
        categories = []
        try:
            cats = self.driver.find_elements(By.CSS_SELECTOR, "div.css-k0zb2y > span")
            for cat in cats[:4]:
                text = cat.text.split('\n')[0].strip()
                categories.append(text)
        except:
            pass
        return categories

    def crawl(self, url):
        try:
            # URL ì¤‘ë³µ ê²€ì‚¬
            if url in self.existing_urls:
                print(f"â© ì´ë¯¸ ìˆ˜ì§‘ëœ ê¸°ì—…, ìŠ¤í‚µ")
                return

            print(f"ğŸ”— {url} í¬ë¡¤ë§ ì‹œì‘")
            self.driver.get(url)
            time.sleep(random.uniform(3,4))  # ë¡œë”© ëŒ€ê¸°

            company_name = self.driver.find_element(By.CSS_SELECTOR, "div.css-gmlt1i h1 span").text.strip()
            company_intro = self.get_dd("ê¸°ì—…ì†Œê°œ")
            listed = self.get_dd("ìƒì¥ì—¬ë¶€")
            established = self.get_dd("ì„¤ë¦½ì¼ì")

            homepage = None
            try:
                homepage_elem = self.driver.find_element(By.XPATH, "//dt[contains(text(),'í™ˆí˜ì´ì§€')]/following-sibling::dd[1]//a")
                homepage = homepage_elem.get_attribute("href")
            except:
                pass

            address_full = self.get_dd("ì£¼ì†Œ")
            address = " ".join(address_full.split()[:2]) if address_full else None

            major_info = self.parse_major_info()
            profit_data = self.parse_table("profit")
            finance_data = self.parse_table("finance")
            news = self.parse_news()
            patents = self.parse_patents()
            categories = self.parse_categories()
            invest_data = self.parse_investment()

            data = {
                "url": url,
                "ê¸°ì—…ëª…": company_name,
                "ê¸°ì—…ì†Œê°œ": company_intro,
                "ìƒì¥ì—¬ë¶€": listed,
                "ì„¤ë¦½ì¼": established,
                "í™ˆí˜ì´ì§€": homepage,
                "ì£¼ì†Œ": address,
                "ì¹´í…Œê³ ë¦¬": categories,
                "ì£¼ìš”ì •ë³´": major_info,
                "íˆ¬ììœ ì¹˜ì •ë³´": invest_data,
                "ì†ìµ": profit_data,
                "ì¬ë¬´": finance_data,
                "ë³´ë„ìë£Œ": news,
                "íŠ¹í—ˆëª…ì¹­ë¦¬ìŠ¤íŠ¸": ", ".join(patents) if patents else None
            }

            self.result.append(data)
            self.existing_urls.add(url)   # â­ï¸ ì—¬ê¸° ì¤‘ìš”
            self.save()
            self.result.clear()
            print(f"âœ… {company_name} ìˆ˜ì§‘ ì™„ë£Œ")

        except Exception as e:
            print(f"âš  {url} ì˜¤ë¥˜ ë°œìƒ â†’ {e}")
            self.error_list.append({"url": url, "error": str(e)})

    def save(self):
        if os.path.exists(self.output_file):
            with open(self.output_file, 'r', encoding='utf-8') as f:
                content = f.read()
                existing = json.loads(content) if content.strip() else []
                self.existing_urls = set([comp["url"] for comp in existing])
        else:
            existing = []

        existing.extend(self.result)

        with open(self.output_file, 'w', encoding='utf-8') as f:
            json.dump(existing, f, ensure_ascii=False, indent=4)
        print(f"ğŸ’¾ ì €ì¥ ì™„ë£Œ: {self.output_file}")

        if self.error_list:
            with open(self.error_log_file, 'w', encoding='utf-8') as f:
                json.dump(self.error_list, f, ensure_ascii=False, indent=4)
            print(f"âš  ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡ë¨: {self.error_log_file}")

    def close(self):
        self.driver.quit()

if __name__ == "__main__":
    crawler = InnoForestCrawler()

    with open("listing.txt", "r", encoding="utf-8") as f:
        urls = [line.strip() for line in f if line.strip()]

    for idx, url in enumerate(urls, 1):
        print(f"ğŸš© {idx} / {len(urls)} ë²ˆì§¸ URL ì§„í–‰ ì¤‘")
        crawler.crawl(url)
        if idx % 50 == 0:
            crawler.driver.quit()
            time.sleep(1)
            crawler.__init__()

    crawler.save()
    crawler.close()










